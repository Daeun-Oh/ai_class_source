{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedd067b",
   "metadata": {},
   "source": [
    "# LeNet-5 (1998, Yann LeCun)\n",
    "- 활성화 함수: Sigmoid\n",
    "- 풀링 방식: AveragePooling(평균 풀링) 사용\n",
    "- 얕은 구조, 손글씨(MNIST) 인식에 사용\n",
    "※ ImageNet 대회 이전 (참고: ImageNet 대회는 2010~2017년에 열림)\n",
    "\n",
    "# AlexNet (2012, ImageNet 대회 우승)\n",
    "- 활성화 함수: ReLU 도입\n",
    "- 풀링 방식: MaxPooling(최대 풀링)\n",
    "- 정규화 기법: Dropout 사용\n",
    "  - 완전 연결층(Dense Layer)에서 일부 유닛을 랜덤하게 끔 → 과적합 방지\n",
    "- GPU 병렬 처리로 대규모 이미지 분류 도전\n",
    "\n",
    "# VGGNet-16 모델 (2014, ImageNet 대회 준우승)\n",
    "- 기존 \"합성곱층 → 풀링층 → 합성곱층 → 풀링층\" 구조 ⟶ \"합성곱층 x n → 풀링층\" 구조로 재구성\n",
    "- 네트워크는 **블록 단위**로 구성\\\n",
    ": \"(3x3 합성곱 x n회) → MaxPooling\" 형태 반복\n",
    "- 하나의 큰 필터(7x7, 5x5)를 사용 ⟶ 여러 개의 3x3 작은 필터를 연속으로 사용\\\n",
    ": <span style=\"color:limegreen\"><b>파라미터 수 절약</b></span>, 비선형성 더 많이 삽입 → 표현력과 성능 향상\n",
    "\n",
    "[VGGNet 예제 바로가기](../day12/vggnet.ipynb)<br>\n",
    "<span style=\"font-size:13px\">강아지 이미지 결과 - ('n02099712', 'Labrador_retriever', np.float32(0.35698158))</span><br>\n",
    "<span style=\"font-size:13px\">고양이 이미지 결과 - ('n02123045', 'tabby', np.float32(0.43275306))</span>\n",
    "\n",
    "> 📌 <span style=\"color:limegreen\"><b>파라미터 수를 줄이는 것</b></span>은 속도와 효율 면에서 매우 중요해짐.\n",
    "\n",
    "# GoogLeNet (2014, ImageNet 대회 우승)\n",
    "- **Inception 모듈** 도입\\\n",
    "  → 서로 다른 크기의 커널(1x1, 3x3, 5x5)을 병렬 적용하여 다양한 특징 추출\n",
    "- 연산 효율 높이고, 파라미터 수를 줄이면서도 높은 성능 확보\n",
    "\n",
    "# ResNet (2015, ImageNet 대회 우승)\n",
    "- 잔차 블록 (Residual Block 또는 Skip Block) 도입 → 정보 전달 경로 보장\n",
    "  - 입력을 출력에 그대로 더하는 **스킵 연결(Skip Connection)**을 통해, 신경망이 입력과 출력의 차이(잔차, Residual)를 학습하도록 유도.\n",
    "  - 이를 통해 정보가 층을 건너뛰며 전달되므로, 깊은 네트워크에서도 학습이 잘 이루어짐.\n",
    "  - **유용한 특징이 층을 거치며 왜곡되거나 약해지는 것**을 막아줌.\n",
    "- 활성화 함수: ReLU\n",
    "- 정규화 기법: 배치 정규화 (Batch Normalization)\n",
    "  - 학습 도중 각 층의 입력 분포가 계속 변하는 문제(Internal Covariate Shift)를 완화.\n",
    "  - 전체 데이터가 아닌, **미니배치 단위**로 정규화 수행. (학습 도중에 정규화)\n",
    "  - 학습 속도 ↑, 과적합 ↓\n",
    "- Flatten 대신 GlobalAveragePooling2D 사용\\\n",
    "  → <span style=\"color:limegreen\"><b>파라미터 감소</b></span>, 과적합 방지\n",
    "\n",
    "# DenseNet\n",
    "- ResNet의 확장 모델\n",
    "- 모든 이전 층의 출력을 현재 층에 직접 연결 (밀집 연결)\n",
    "- 파라미터 효율 + 정보 흐름 개선\n",
    "\n",
    "# MobileNet (2017~) - 모바일 환경(경량) 모델\n",
    "- 경량 모델 (모바일/임베디드 최적화)\n",
    "- Depthwise Separable Convolution 사용\\\n",
    "  → 깊이별 합성곱 + 점별 합성곱\n",
    "- 연산량↓, 속도↑, 경량화(파라미터 수 축소)에 초점\n",
    "\n",
    "# EfficientNet (2019, Google) ✨\n",
    "- **Compound Scaling**: 모델의 깊이(depth), 너비(width), 해상도(resolution)를 **균형 있게** 조절\n",
    "- 활성화 함수: **Swish(스위시) 함수** 사용 (ReLU보다 부드럽고 성능 우수)\n",
    "- 높은 정확도 + 낮은 연산량 (SOTA 효율 달성)\n",
    "\n",
    "[EfficientNet 예제 (1) 바로가기](../day12/efficientnet1.ipynb)<br>\n",
    "<span style=\"font-size:13px\">강아지 이미지 결과 - ('n02099712', 'Labrador_retriever', np.float32(0.3682936))</span><br>\n",
    "<span style=\"font-size:13px\">고양이 이미지 결과 - ('n02124075', 'Egyptian_cat', np.float32(0.35218692))</span>\n",
    "\n",
    "[EfficientNet 예제 (2) 바로가기](../day12/efficientnet1.ipynb)<br>\n",
    "<span style=\"font-size:13px\">피스타치오 이미지 결과 - ('n01955084', 'chiton', np.float32(0.25582632))</span>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
